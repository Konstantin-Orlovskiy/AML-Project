{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#you need the pima_indians.csv to run\n",
    "\n",
    "#label data\n",
    "\n",
    "filename = 'FeatureSelectionOutput.csv'\n",
    "\n",
    "\n",
    "#read data\n",
    "data = read_csv(filename)\n",
    "array = data.values\n",
    "X = array[:,0:-1]\n",
    "y = array[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "# split data and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)\n",
    "mlp = MLPClassifier()\n",
    "df = pd.DataFrame(y_test, columns = ['label'])\n",
    "\n",
    "\n",
    "\n",
    "# fit model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# make class predictions for the testing set\n",
    "y_pred_class = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#pass true and predicted values into dataframe\n",
    "#df = y_test.to_frame().reset_index()\n",
    "df['predicted'] = y_pred_class\n",
    "df['probabilty'] = y_pred_prob\n",
    "#df.shape()\n",
    "\n",
    "#if you want to check data has read in correctly:\n",
    "#df.head()\n",
    "\n",
    "#to test function:\n",
    "#model_evaluator(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "##Evaluates the performance of a binary classifier against labelled data.\n",
    "# @param data - a dataframe containing 3 columns: class (binary values), predicted(binary values) and \n",
    "#probability (the predicition the model has made for each value, prior to converting to a binary classification, number between 0 and 1).\n",
    "# @returns - Confusion matrix plot, ROC curve plot, and a small report.\n",
    "\n",
    "def model_evaluator(data):\n",
    "    \n",
    "    #partition processed data into vectors\n",
    "    actualClass = data.label\n",
    "    predictedClass = data.predicted\n",
    "    probability = data.probabilty \n",
    "    \n",
    "    #build a confusion matrix\n",
    "    cm = confusion_matrix(actualClass, predictedClass)\n",
    "    \n",
    "    \n",
    "    TruePositive = cm[1, 1]\n",
    "    TrueNegative = cm[0,0]\n",
    "    FalsePositive = cm[0,1]\n",
    "    FalseNegative = cm[1,0]\n",
    "    \n",
    "    numberOfPositives = TruePositive + FalseNegative\n",
    "    numberOfNegatives = TrueNegative + FalsePositive\n",
    "    \n",
    "    #calculate the Null accuracy\n",
    "    null_accuracy = 1 - actualClass.mean()\n",
    "    \n",
    "    #define the model accuracy\n",
    "    model_accuracy = metrics.accuracy_score(actualClass, predictedClass)\n",
    "    \n",
    "    #Generate a metrics report\n",
    "    report = metrics.classification_report(actualClass, predictedClass, output_dict = True)\n",
    "    #print(report)\n",
    "    \n",
    "    #calculate the model performance over the null accuracy\n",
    "    performance_over_null = model_accuracy - null_accuracy\n",
    "    \n",
    "    #Calculate the Specificity of the model \n",
    "    specificity = TrueNegative / (TrueNegative + FalsePositive)\n",
    "    \n",
    "    #Calculate the True positive rate, false positive rate, and thresholds to plot a rock curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actualClass, probability)\n",
    "    \n",
    "    #Calculate the Area under the ROC Curve\n",
    "    rocAuc = metrics.roc_auc_score(actualClass, probability)\n",
    "    \n",
    "    #generate figure\n",
    "    fig = plt.figure(figsize=(12, 20))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=1, wspace=0.5, figure=fig)\n",
    "    \n",
    "    #plot confusion matrix in pos 0,0\n",
    "    confusionMatrixLabels = ['Normal Traffic', 'Intrusion']\n",
    "    confusionMatrixColourMap = plt.cm.Blues\n",
    "    confusionMatrix = fig.add_subplot(spec[0, 0])\n",
    "    confusionMatrix.set_aspect('equal')\n",
    "    confusionMatrix.imshow(cm, interpolation='nearest', cmap = confusionMatrixColourMap)\n",
    "    confusionMatrix.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), \n",
    "                         xticklabels = confusionMatrixLabels, yticklabels = confusionMatrixLabels,\n",
    "                         ylabel='True class', xlabel='Predicted class')\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            confusionMatrix.text(j, i, (format(cm[i, j])), ha=\"center\", va=\"center\", color=\"red\", size = 'larger')\n",
    "    \n",
    "    cmLabels = ['TN', 'FP', 'FN', 'TP' ]\n",
    "    a = 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            confusionMatrix.text(j + 0.3, i + 0.4, (cmLabels[a]), ha ='center', va=\"baseline\", color = 'red', size = 'larger')\n",
    "            if a < 4:\n",
    "                a += 1\n",
    "    a=0          \n",
    "    for i in range(cm.shape[a]):\n",
    "        if a == 0:\n",
    "            confusionMatrix.text(j+0.8, i, ('Total:\\n %d' % (numberOfNegatives)), ha ='center', va=\"center\", color = 'black', size = 'larger')\n",
    "            a += 1\n",
    "        else:\n",
    "            confusionMatrix.text(j+0.8, i, ('Total:\\n %d' % (numberOfPositives)), ha ='center', va=\"center\", color = 'black', size = 'larger')\n",
    "    \n",
    "    #plot roc curve in position 0,1 \n",
    "    rocCurve = fig.add_subplot(spec[0, 1])\n",
    "    rocCurve.set_aspect('equal')\n",
    "    rocCurve.plot(fpr, tpr, color='red', lw=2, label = 'ROC area = %0.2f)' % rocAuc )\n",
    "    rocCurve.set(xlim = [0.0, 1.0], ylim = [0.0, 1.0], xlabel = 'False Positive Rate (1 - Specificity)', ylabel = 'True Positive Rate (Sensitivity)' )\n",
    "    rocCurve.legend(loc=\"lower right\")\n",
    "    \n",
    "    #print report\n",
    "    print('The performance of this model over the null accuracy is %2.2f%%\\nModel Sensitivity: %2.2f \\nModel Specificity: %2.2f \\nModel F1 Score: %2.2f' \n",
    "      % ((performance_over_null *100), (report['1.0']['recall']), (specificity), (report['1.0']['f1-score'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_evaluator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
